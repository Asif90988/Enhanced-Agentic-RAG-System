"""
Enhanced Plan-Execute Agent with Real-time Coordination

Coordinates the execution of complex multi-step plans with real-time data integration,
managing the workflow between planning, execution, and synthesis phases.
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from storage.document_repository import DocumentRepository
from .enhanced_routing_agent import EnhancedRoutingAgent
from .enhanced_query_planning_agent import EnhancedQueryPlanningAgent
from .enhanced_react_agent import EnhancedReActAgent

logger = logging.getLogger(__name__)

class EnhancedPlanExecuteAgent:
    """
    Enhanced plan-execute agent that orchestrates complex query processing workflows.
    
    Features:
    - Multi-step plan execution with real-time coordination
    - Parallel execution of independent tasks
    - Dynamic plan adjustment based on intermediate results
    - Progress tracking and monitoring
    - Error handling and recovery
    - Result synthesis and quality assessment
    """
    
    def __init__(self, document_repository: DocumentRepository):
        """Initialize enhanced plan-execute agent."""
        self.document_repository = document_repository
        
        # Initialize sub-agents
        self.routing_agent = EnhancedRoutingAgent(document_repository)
        self.planning_agent = EnhancedQueryPlanningAgent(document_repository, self.routing_agent)
        self.react_agent = EnhancedReActAgent(document_repository)
        
        # Execution configuration
        self.max_parallel_tasks = 4
        self.execution_timeout = 300  # 5 minutes
        self.retry_attempts = 2
        
        logger.info("Enhanced plan-execute agent initialized")
    
    def execute_query(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Execute a complete query processing workflow.
        
        Args:
            query: User query to process
            context: Additional context information
            
        Returns:
            Complete execution results with final answer
        """
        try:
            # Initialize execution state
            execution_state = self._initialize_execution_state(query, context)
            
            # Phase 1: Planning
            planning_result = self._execute_planning_phase(execution_state)
            execution_state['planning_result'] = planning_result
            
            # Phase 2: Execution
            execution_result = self._execute_execution_phase(execution_state)
            execution_state['execution_result'] = execution_result
            
            # Phase 3: Synthesis
            synthesis_result = self._execute_synthesis_phase(execution_state)
            execution_state['synthesis_result'] = synthesis_result
            
            # Generate final response
            final_response = self._generate_final_response(execution_state)
            execution_state['final_response'] = final_response
            
            logger.info(f"Query execution completed successfully")
            return execution_state
            
        except Exception as e:
            logger.error(f"Error in query execution: {e}")
            return self._get_fallback_execution_result(query, str(e))
    
    def _initialize_execution_state(self, query: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Initialize the execution state for query processing."""
        return {
            'query': query,
            'context': context or {},
            'start_time': datetime.utcnow().isoformat(),
            'phases': {
                'planning': {'status': 'pending', 'start_time': None, 'end_time': None},
                'execution': {'status': 'pending', 'start_time': None, 'end_time': None},
                'synthesis': {'status': 'pending', 'start_time': None, 'end_time': None}
            },
            'step_results': {},
            'errors': [],
            'performance_metrics': {
                'total_documents_searched': 0,
                'total_search_time': 0.0,
                'cache_hits': 0,
                'api_calls': 0
            }
        }
    
    def _execute_planning_phase(self, execution_state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the planning phase."""
        try:
            self._update_phase_status(execution_state, 'planning', 'running')
            
            query = execution_state['query']
            context = execution_state['context']
            
            # Get routing decision
            routing_decision = self.routing_agent.route_query(query, context)
            
            # Create execution plan
            execution_plan = self.planning_agent.plan_query_execution(query, context)
            
            # Validate plan
            validation_result = self.planning_agent.validate_plan(execution_plan)
            
            planning_result = {
                'routing_decision': routing_decision,
                'execution_plan': execution_plan,
                'validation_result': validation_result,
                'planning_confidence': routing_decision.get('routing_confidence', 0.5)
            }
            
            self._update_phase_status(execution_state, 'planning', 'completed')
            return planning_result
            
        except Exception as e:
            self._update_phase_status(execution_state, 'planning', 'failed')
            execution_state['errors'].append(f"Planning phase failed: {e}")
            raise
    
    def _execute_execution_phase(self, execution_state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the main execution phase."""
        try:
            self._update_phase_status(execution_state, 'execution', 'running')
            
            planning_result = execution_state['planning_result']
            execution_plan = planning_result['execution_plan']
            
            if execution_plan.get('plan_type') == 'single_step':
                # Execute single step plan
                execution_result = self._execute_single_step_plan(execution_plan, execution_state)
            else:
                # Execute multi-step plan
                execution_result = self._execute_multi_step_plan(execution_plan, execution_state)
            
            self._update_phase_status(execution_state, 'execution', 'completed')
            return execution_result
            
        except Exception as e:
            self._update_phase_status(execution_state, 'execution', 'failed')
            execution_state['errors'].append(f"Execution phase failed: {e}")
            raise
    
    def _execute_single_step_plan(self, execution_plan: Dict[str, Any], execution_state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single-step plan."""
        try:
            step = execution_plan['steps'][0]
            query = execution_state['query']
            
            # Execute using ReAct agent for comprehensive processing
            react_result = self.react_agent.execute_reasoning_cycle(query)
            
            return {
                'plan_type': 'single_step',
                'step_results': {1: react_result},
                'total_steps_executed': 1,
                'execution_time': (datetime.utcnow() - datetime.fromisoformat(execution_state['start_time'])).total_seconds()
            }
            
        except Exception as e:
            logger.error(f"Error executing single step plan: {e}")
            raise
    
    def _execute_multi_step_plan(self, execution_plan: Dict[str, Any], execution_state: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a multi-step plan with parallel processing where possible."""
        try:
            steps = execution_plan['steps']\n            parallel_groups = execution_plan.get('parallel_execution', [])\n            step_results = {}\n            \n            # Execute steps in dependency order\n            executed_steps = set()\n            \n            while len(executed_steps) < len(steps):\n                # Find steps that can be executed (dependencies satisfied)\n                ready_steps = []\n                for step in steps:\n                    step_id = step['step_id']\n                    if step_id not in executed_steps:\n                        dependencies = step.get('dependencies', [])\n                        if all(dep in executed_steps for dep in dependencies):\n                            ready_steps.append(step)\n                \n                if not ready_steps:\n                    break  # No more steps can be executed\n                \n                # Check if ready steps can be executed in parallel\n                parallel_ready = self._group_parallel_steps(ready_steps, parallel_groups)\n                \n                if len(parallel_ready) > 1:\n                    # Execute in parallel\n                    parallel_results = self._execute_steps_parallel(parallel_ready, execution_state)\n                    step_results.update(parallel_results)\n                    executed_steps.update(step['step_id'] for step in parallel_ready)\n                else:\n                    # Execute single step\n                    step = ready_steps[0]\n                    result = self._execute_single_step(step, execution_state, step_results)\n                    step_results[step['step_id']] = result\n                    executed_steps.add(step['step_id'])\n            \n            return {\n                'plan_type': 'multi_step',\n                'step_results': step_results,\n                'total_steps_executed': len(executed_steps),\n                'execution_time': (datetime.utcnow() - datetime.fromisoformat(execution_state['start_time'])).total_seconds()\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error executing multi-step plan: {e}\")\n            raise\n    \n    def _group_parallel_steps(self, ready_steps: List[Dict[str, Any]], parallel_groups: List[List[int]]) -> List[Dict[str, Any]]:\n        \"\"\"Group ready steps that can be executed in parallel.\"\"\"\n        if not parallel_groups:\n            return ready_steps[:1]  # Execute one at a time if no parallel groups\n        \n        ready_step_ids = {step['step_id'] for step in ready_steps}\n        \n        # Find the largest parallel group that's ready\n        for group in parallel_groups:\n            group_ready = [step for step in ready_steps if step['step_id'] in group]\n            if len(group_ready) > 1:\n                return group_ready\n        \n        return ready_steps[:1]  # Default to single step\n    \n    def _execute_steps_parallel(self, steps: List[Dict[str, Any]], execution_state: Dict[str, Any]) -> Dict[int, Dict[str, Any]]:\n        \"\"\"Execute multiple steps in parallel.\"\"\"\n        try:\n            results = {}\n            \n            with ThreadPoolExecutor(max_workers=min(len(steps), self.max_parallel_tasks)) as executor:\n                # Submit all steps for execution\n                future_to_step = {\n                    executor.submit(self._execute_single_step, step, execution_state, {}): step\n                    for step in steps\n                }\n                \n                # Collect results\n                for future in as_completed(future_to_step, timeout=self.execution_timeout):\n                    step = future_to_step[future]\n                    try:\n                        result = future.result()\n                        results[step['step_id']] = result\n                    except Exception as e:\n                        logger.error(f\"Error executing step {step['step_id']}: {e}\")\n                        results[step['step_id']] = {\n                            'error': str(e),\n                            'success': False,\n                            'step_type': step.get('type', 'unknown')\n                        }\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error in parallel execution: {e}\")\n            raise\n    \n    def _execute_single_step(self, step: Dict[str, Any], execution_state: Dict[str, Any], \n                            previous_results: Dict[int, Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Execute a single step in the plan.\"\"\"\n        try:\n            step_type = step.get('type', 'direct_search')\n            step_query = step.get('query', execution_state['query'])\n            search_strategy = step.get('search_strategy', 'hybrid')\n            sources = step.get('sources', [])\n            \n            start_time = datetime.utcnow()\n            \n            if step_type == 'synthesis':\n                # Synthesis step - combine results from previous steps\n                result = self._execute_synthesis_step(step, previous_results, execution_state)\n            elif step_type == 'real_time_update':\n                # Real-time update step\n                result = self._execute_real_time_step(step, execution_state)\n            else:\n                # Regular search step\n                result = self._execute_search_step(step, execution_state)\n            \n            # Add timing information\n            execution_time = (datetime.utcnow() - start_time).total_seconds()\n            result['execution_time'] = execution_time\n            result['step_id'] = step['step_id']\n            result['step_type'] = step_type\n            \n            # Update performance metrics\n            self._update_performance_metrics(execution_state, result)\n            \n            return result\n            \n        except Exception as e:\n            logger.error(f\"Error executing step {step.get('step_id')}: {e}\")\n            return {\n                'error': str(e),\n                'success': False,\n                'step_id': step.get('step_id'),\n                'step_type': step.get('type', 'unknown')\n            }\n    \n    def _execute_search_step(self, step: Dict[str, Any], execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a search step.\"\"\"\n        try:\n            query = step.get('query', execution_state['query'])\n            search_strategy = step.get('search_strategy', 'hybrid')\n            sources = step.get('sources', [])\n            \n            # Determine source filter\n            source_filter = None\n            if sources:\n                # Use the highest priority source type\n                top_source = max(sources, key=lambda x: x.get('priority', 0))\n                source_filter = top_source.get('source_type')\n            \n            # Execute search\n            results = self.document_repository.search_documents(\n                query=query,\n                search_type=search_strategy,\n                top_k=10,\n                source_filter=source_filter\n            )\n            \n            return {\n                'success': True,\n                'results': results,\n                'result_count': len(results),\n                'search_strategy': search_strategy,\n                'source_filter': source_filter,\n                'query_used': query\n            }\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    def _execute_synthesis_step(self, step: Dict[str, Any], previous_results: Dict[int, Dict[str, Any]], \n                               execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a synthesis step.\"\"\"\n        try:\n            # Collect all results from previous steps\n            all_results = []\n            for step_id, result in previous_results.items():\n                if result.get('success', False) and 'results' in result:\n                    all_results.extend(result['results'])\n            \n            # Group and analyze results\n            synthesis = {\n                'total_documents': len(all_results),\n                'source_breakdown': self._analyze_source_breakdown(all_results),\n                'key_findings': self._extract_key_findings(all_results),\n                'confidence_assessment': self._assess_synthesis_confidence(all_results),\n                'information_coverage': self._assess_information_coverage(all_results, execution_state['query'])\n            }\n            \n            return {\n                'success': True,\n                'synthesis': synthesis,\n                'documents_synthesized': len(all_results)\n            }\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    def _execute_real_time_step(self, step: Dict[str, Any], execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a real-time update step.\"\"\"\n        try:\n            query = step.get('query', execution_state['query'])\n            \n            # Get recent documents\n            recent_docs = self.document_repository.get_recent_documents(limit=20)\n            \n            # Filter for relevance\n            relevant_docs = []\n            for doc in recent_docs:\n                relevance = self._calculate_document_relevance(doc, query)\n                if relevance > 0.3:\n                    doc['relevance_score'] = relevance\n                    relevant_docs.append(doc)\n            \n            # Sort by relevance and recency\n            relevant_docs.sort(key=lambda x: (x['relevance_score'], x.get('created_at', '')), reverse=True)\n            \n            return {\n                'success': True,\n                'results': relevant_docs[:10],\n                'total_recent_docs': len(recent_docs),\n                'relevant_count': len(relevant_docs)\n            }\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    def _calculate_document_relevance(self, document: Dict[str, Any], query: str) -> float:\n        \"\"\"Calculate relevance of document to query.\"\"\"\n        try:\n            doc_text = (\n                document.get('title', '') + ' ' +\n                document.get('content_summary', '') + ' ' +\n                document.get('content_text', '')[:500]\n            ).lower()\n            \n            query_terms = query.lower().split()\n            matches = sum(1 for term in query_terms if term in doc_text)\n            \n            return matches / len(query_terms) if query_terms else 0.0\n            \n        except Exception:\n            return 0.0\n    \n    def _execute_synthesis_phase(self, execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute the synthesis phase.\"\"\"\n        try:\n            self._update_phase_status(execution_state, 'synthesis', 'running')\n            \n            execution_result = execution_state['execution_result']\n            step_results = execution_result.get('step_results', {})\n            \n            # Collect all documents and evidence\n            all_documents = []\n            all_evidence = []\n            \n            for step_id, result in step_results.items():\n                if result.get('success', False):\n                    # From search results\n                    if 'results' in result:\n                        all_documents.extend(result['results'])\n                    \n                    # From ReAct evidence\n                    if 'evidence' in result:\n                        all_evidence.extend(result['evidence'])\n                    \n                    # From synthesis results\n                    if 'synthesis' in result:\n                        synthesis_data = result['synthesis']\n                        if 'key_findings' in synthesis_data:\n                            all_evidence.extend(synthesis_data['key_findings'])\n            \n            # Generate comprehensive synthesis\n            synthesis_result = {\n                'final_synthesis': self._generate_comprehensive_synthesis(all_documents, all_evidence, execution_state),\n                'quality_assessment': self._assess_result_quality(all_documents, all_evidence),\n                'source_analysis': self._analyze_sources(all_documents),\n                'confidence_metrics': self._calculate_confidence_metrics(all_documents, all_evidence)\n            }\n            \n            self._update_phase_status(execution_state, 'synthesis', 'completed')\n            return synthesis_result\n            \n        except Exception as e:\n            self._update_phase_status(execution_state, 'synthesis', 'failed')\n            execution_state['errors'].append(f\"Synthesis phase failed: {e}\")\n            raise\n    \n    def _generate_comprehensive_synthesis(self, documents: List[Dict[str, Any]], \n                                        evidence: List[Dict[str, Any]], \n                                        execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive synthesis of all collected information.\"\"\"\n        try:\n            query = execution_state['query']\n            \n            # Extract key information\n            key_points = []\n            supporting_evidence = []\n            \n            # From documents\n            for doc in documents:\n                if doc.get('content_summary'):\n                    key_points.append({\n                        'content': doc['content_summary'],\n                        'source': doc.get('source_type', 'unknown'),\n                        'relevance': doc.get('relevance_score', 0.0),\n                        'document_id': doc.get('id')\n                    })\n            \n            # From evidence\n            for item in evidence:\n                if isinstance(item, dict) and item.get('content'):\n                    supporting_evidence.append({\n                        'content': item['content'],\n                        'source': item.get('source_type', 'unknown'),\n                        'confidence': item.get('relevance_score', 0.0)\n                    })\n            \n            # Sort by relevance/confidence\n            key_points.sort(key=lambda x: x.get('relevance', 0), reverse=True)\n            supporting_evidence.sort(key=lambda x: x.get('confidence', 0), reverse=True)\n            \n            # Generate answer\n            answer_components = []\n            if key_points:\n                answer_components.append(\"Based on the available information:\")\n                for i, point in enumerate(key_points[:5], 1):\n                    answer_components.append(f\"{i}. {point['content'][:200]}...\")\n            \n            answer = \" \".join(answer_components) if answer_components else \"No sufficient information found.\"\n            \n            return {\n                'answer': answer,\n                'key_points': key_points[:10],\n                'supporting_evidence': supporting_evidence[:10],\n                'total_sources': len(set(p.get('source') for p in key_points + supporting_evidence)),\n                'synthesis_confidence': self._calculate_synthesis_confidence(key_points, supporting_evidence)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error generating synthesis: {e}\")\n            return {\n                'answer': 'Error occurred during synthesis.',\n                'error': str(e)\n            }\n    \n    def _calculate_synthesis_confidence(self, key_points: List[Dict[str, Any]], \n                                      supporting_evidence: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate confidence in the synthesis.\"\"\"\n        try:\n            if not key_points and not supporting_evidence:\n                return 0.0\n            \n            # Factor 1: Number of sources\n            all_items = key_points + supporting_evidence\n            source_count = len(set(item.get('source', 'unknown') for item in all_items))\n            source_factor = min(source_count / 3.0, 1.0)\n            \n            # Factor 2: Average relevance/confidence\n            relevance_scores = []\n            for item in key_points:\n                relevance_scores.append(item.get('relevance', 0.0))\n            for item in supporting_evidence:\n                relevance_scores.append(item.get('confidence', 0.0))\n            \n            avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0\n            \n            # Factor 3: Information quantity\n            quantity_factor = min(len(all_items) / 10.0, 1.0)\n            \n            # Combined confidence\n            confidence = (source_factor * 0.4 + avg_relevance * 0.4 + quantity_factor * 0.2)\n            \n            return min(confidence, 1.0)\n            \n        except Exception:\n            return 0.5\n    \n    def _analyze_source_breakdown(self, documents: List[Dict[str, Any]]) -> Dict[str, int]:\n        \"\"\"Analyze breakdown of documents by source type.\"\"\"\n        breakdown = {}\n        for doc in documents:\n            source_type = doc.get('source_type', 'unknown')\n            breakdown[source_type] = breakdown.get(source_type, 0) + 1\n        return breakdown\n    \n    def _extract_key_findings(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Extract key findings from documents.\"\"\"\n        findings = []\n        for doc in documents:\n            if doc.get('content_summary') and doc.get('relevance_score', 0) > 0.5:\n                findings.append({\n                    'content': doc['content_summary'],\n                    'source': doc.get('source_type'),\n                    'relevance': doc.get('relevance_score', 0.0),\n                    'document_id': doc.get('id')\n                })\n        \n        # Sort by relevance and return top findings\n        findings.sort(key=lambda x: x['relevance'], reverse=True)\n        return findings[:10]\n    \n    def _assess_synthesis_confidence(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Assess confidence in synthesis results.\"\"\"\n        if not documents:\n            return {'overall': 0.0, 'factors': {}}\n        \n        relevance_scores = [doc.get('relevance_score', 0) for doc in documents]\n        source_types = set(doc.get('source_type') for doc in documents)\n        \n        factors = {\n            'document_count': len(documents),\n            'average_relevance': sum(relevance_scores) / len(relevance_scores),\n            'source_diversity': len(source_types),\n            'high_relevance_count': sum(1 for score in relevance_scores if score > 0.7)\n        }\n        \n        # Calculate overall confidence\n        overall = (\n            min(factors['document_count'] / 10.0, 1.0) * 0.3 +\n            factors['average_relevance'] * 0.4 +\n            min(factors['source_diversity'] / 3.0, 1.0) * 0.2 +\n            min(factors['high_relevance_count'] / 5.0, 1.0) * 0.1\n        )\n        \n        return {\n            'overall': overall,\n            'factors': factors\n        }\n    \n    def _assess_information_coverage(self, documents: List[Dict[str, Any]], query: str) -> Dict[str, Any]:\n        \"\"\"Assess how well the information covers the query.\"\"\"\n        # Simplified implementation\n        query_terms = set(query.lower().split())\n        covered_terms = set()\n        \n        for doc in documents:\n            doc_text = (doc.get('title', '') + ' ' + doc.get('content_summary', '')).lower()\n            for term in query_terms:\n                if term in doc_text:\n                    covered_terms.add(term)\n        \n        coverage_ratio = len(covered_terms) / len(query_terms) if query_terms else 0.0\n        \n        return {\n            'coverage_ratio': coverage_ratio,\n            'covered_terms': list(covered_terms),\n            'missing_terms': list(query_terms - covered_terms),\n            'total_documents': len(documents)\n        }\n    \n    def _assess_result_quality(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Assess overall quality of results.\"\"\"\n        quality_metrics = {\n            'completeness': self._assess_completeness(documents, evidence),\n            'reliability': self._assess_reliability(documents, evidence),\n            'recency': self._assess_recency(documents),\n            'diversity': self._assess_diversity(documents)\n        }\n        \n        # Calculate overall quality score\n        overall_quality = sum(quality_metrics.values()) / len(quality_metrics)\n        \n        return {\n            'overall_quality': overall_quality,\n            'metrics': quality_metrics,\n            'quality_grade': self._get_quality_grade(overall_quality)\n        }\n    \n    def _assess_completeness(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> float:\n        \"\"\"Assess completeness of information.\"\"\"\n        total_items = len(documents) + len(evidence)\n        if total_items >= 10:\n            return 1.0\n        elif total_items >= 5:\n            return 0.8\n        elif total_items >= 2:\n            return 0.6\n        else:\n            return 0.3\n    \n    def _assess_reliability(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> float:\n        \"\"\"Assess reliability of sources.\"\"\"\n        # Simplified - in practice, you'd have source reliability scores\n        reliable_sources = {'news_connector', 'file_connector'}\n        \n        reliable_count = 0\n        total_count = 0\n        \n        for doc in documents:\n            total_count += 1\n            if doc.get('source_type') in reliable_sources:\n                reliable_count += 1\n        \n        return reliable_count / total_count if total_count > 0 else 0.5\n    \n    def _assess_recency(self, documents: List[Dict[str, Any]]) -> float:\n        \"\"\"Assess recency of information.\"\"\"\n        if not documents:\n            return 0.0\n        \n        now = datetime.utcnow()\n        recent_count = 0\n        \n        for doc in documents:\n            created_at = doc.get('created_at')\n            if created_at:\n                try:\n                    if isinstance(created_at, str):\n                        created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))\n                    \n                    age_hours = (now - created_at).total_seconds() / 3600\n                    if age_hours < 168:  # Within a week\n                        recent_count += 1\n                except Exception:\n                    pass\n        \n        return recent_count / len(documents)\n    \n    def _assess_diversity(self, documents: List[Dict[str, Any]]) -> float:\n        \"\"\"Assess diversity of sources.\"\"\"\n        if not documents:\n            return 0.0\n        \n        source_types = set(doc.get('source_type') for doc in documents)\n        max_diversity = 3  # Assuming 3 main source types\n        \n        return min(len(source_types) / max_diversity, 1.0)\n    \n    def _get_quality_grade(self, quality_score: float) -> str:\n        \"\"\"Convert quality score to grade.\"\"\"\n        if quality_score >= 0.9:\n            return 'Excellent'\n        elif quality_score >= 0.8:\n            return 'Good'\n        elif quality_score >= 0.6:\n            return 'Fair'\n        else:\n            return 'Poor'\n    \n    def _analyze_sources(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Analyze source distribution and characteristics.\"\"\"\n        source_analysis = {\n            'total_sources': len(documents),\n            'source_breakdown': {},\n            'average_relevance_by_source': {},\n            'most_relevant_sources': []\n        }\n        \n        # Group by source type\n        source_groups = {}\n        for doc in documents:\n            source_type = doc.get('source_type', 'unknown')\n            if source_type not in source_groups:\n                source_groups[source_type] = []\n            source_groups[source_type].append(doc)\n        \n        # Analyze each source type\n        for source_type, docs in source_groups.items():\n            source_analysis['source_breakdown'][source_type] = len(docs)\n            \n            relevance_scores = [doc.get('relevance_score', 0) for doc in docs]\n            avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0\n            source_analysis['average_relevance_by_source'][source_type] = avg_relevance\n        \n        # Find most relevant sources\n        all_docs_with_relevance = [(doc, doc.get('relevance_score', 0)) for doc in documents]\n        all_docs_with_relevance.sort(key=lambda x: x[1], reverse=True)\n        \n        source_analysis['most_relevant_sources'] = [\n            {\n                'document_id': doc.get('id'),\n                'source_type': doc.get('source_type'),\n                'relevance_score': relevance,\n                'title': doc.get('title', '')[:100]\n            }\n            for doc, relevance in all_docs_with_relevance[:5]\n        ]\n        \n        return source_analysis\n    \n    def _calculate_confidence_metrics(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Calculate comprehensive confidence metrics.\"\"\"\n        metrics = {\n            'data_confidence': self._calculate_data_confidence(documents, evidence),\n            'source_confidence': self._calculate_source_confidence(documents),\n            'coverage_confidence': self._calculate_coverage_confidence(documents),\n            'consistency_confidence': self._calculate_consistency_confidence(documents, evidence)\n        }\n        \n        # Overall confidence\n        metrics['overall_confidence'] = sum(metrics.values()) / len(metrics)\n        \n        return metrics\n    \n    def _calculate_data_confidence(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate confidence based on data quality.\"\"\"\n        if not documents and not evidence:\n            return 0.0\n        \n        # Average relevance scores\n        all_scores = []\n        for doc in documents:\n            all_scores.append(doc.get('relevance_score', 0.0))\n        for item in evidence:\n            if isinstance(item, dict):\n                all_scores.append(item.get('relevance_score', 0.0))\n        \n        return sum(all_scores) / len(all_scores) if all_scores else 0.0\n    \n    def _calculate_source_confidence(self, documents: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate confidence based on source reliability.\"\"\"\n        if not documents:\n            return 0.0\n        \n        # Simple source reliability mapping\n        source_reliability = {\n            'news_connector': 0.9,\n            'file_connector': 0.8,\n            'rss_connector': 0.7\n        }\n        \n        total_reliability = 0.0\n        for doc in documents:\n            source_type = doc.get('source_type', 'unknown')\n            total_reliability += source_reliability.get(source_type, 0.5)\n        \n        return total_reliability / len(documents)\n    \n    def _calculate_coverage_confidence(self, documents: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate confidence based on information coverage.\"\"\"\n        # Based on number of documents and source diversity\n        doc_count = len(documents)\n        source_types = len(set(doc.get('source_type') for doc in documents))\n        \n        doc_factor = min(doc_count / 10.0, 1.0)\n        diversity_factor = min(source_types / 3.0, 1.0)\n        \n        return (doc_factor + diversity_factor) / 2.0\n    \n    def _calculate_consistency_confidence(self, documents: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> float:\n        \"\"\"Calculate confidence based on information consistency.\"\"\"\n        # Simplified - in practice, you'd check for contradictions\n        # For now, assume higher confidence with more sources\n        total_items = len(documents) + len(evidence)\n        return min(total_items / 15.0, 1.0)\n    \n    def _generate_final_response(self, execution_state: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate the final response to the user.\"\"\"\n        try:\n            synthesis_result = execution_state.get('synthesis_result', {})\n            final_synthesis = synthesis_result.get('final_synthesis', {})\n            quality_assessment = synthesis_result.get('quality_assessment', {})\n            \n            # Extract answer\n            answer = final_synthesis.get('answer', 'No answer could be generated.')\n            \n            # Extract sources\n            sources = []\n            key_points = final_synthesis.get('key_points', [])\n            for point in key_points[:10]:\n                if point.get('document_id'):\n                    sources.append({\n                        'id': point['document_id'],\n                        'type': point.get('source'),\n                        'relevance': point.get('relevance', 0.0)\n                    })\n            \n            # Calculate overall confidence\n            confidence_metrics = synthesis_result.get('confidence_metrics', {})\n            overall_confidence = confidence_metrics.get('overall_confidence', 0.5)\n            \n            # Performance metrics\n            end_time = datetime.utcnow()\n            start_time = datetime.fromisoformat(execution_state['start_time'])\n            total_time = (end_time - start_time).total_seconds()\n            \n            return {\n                'answer': answer,\n                'confidence': overall_confidence,\n                'sources': sources,\n                'quality_grade': quality_assessment.get('quality_grade', 'Unknown'),\n                'execution_summary': {\n                    'total_time': total_time,\n                    'phases_completed': self._count_completed_phases(execution_state),\n                    'total_documents_processed': synthesis_result.get('source_analysis', {}).get('total_sources', 0),\n                    'errors_encountered': len(execution_state.get('errors', []))\n                },\n                'metadata': {\n                    'query': execution_state['query'],\n                    'execution_id': execution_state.get('execution_id', 'unknown'),\n                    'timestamp': end_time.isoformat()\n                }\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error generating final response: {e}\")\n            return {\n                'answer': 'An error occurred while generating the response.',\n                'confidence': 0.0,\n                'sources': [],\n                'error': str(e)\n            }\n    \n    def _count_completed_phases(self, execution_state: Dict[str, Any]) -> int:\n        \"\"\"Count number of completed phases.\"\"\"\n        phases = execution_state.get('phases', {})\n        return sum(1 for phase in phases.values() if phase.get('status') == 'completed')\n    \n    def _update_phase_status(self, execution_state: Dict[str, Any], phase_name: str, status: str):\n        \"\"\"Update the status of a phase.\"\"\"\n        phases = execution_state.setdefault('phases', {})\n        phase = phases.setdefault(phase_name, {})\n        \n        phase['status'] = status\n        \n        if status == 'running':\n            phase['start_time'] = datetime.utcnow().isoformat()\n        elif status in ['completed', 'failed']:\n            phase['end_time'] = datetime.utcnow().isoformat()\n    \n    def _update_performance_metrics(self, execution_state: Dict[str, Any], step_result: Dict[str, Any]):\n        \"\"\"Update performance metrics based on step result.\"\"\"\n        metrics = execution_state.setdefault('performance_metrics', {})\n        \n        if step_result.get('success', False):\n            result_count = step_result.get('result_count', 0)\n            execution_time = step_result.get('execution_time', 0.0)\n            \n            metrics['total_documents_searched'] = metrics.get('total_documents_searched', 0) + result_count\n            metrics['total_search_time'] = metrics.get('total_search_time', 0.0) + execution_time\n            metrics['api_calls'] = metrics.get('api_calls', 0) + 1\n    \n    def _get_fallback_execution_result(self, query: str, error_message: str) -> Dict[str, Any]:\n        \"\"\"Get fallback execution result when main execution fails.\"\"\"\n        return {\n            'query': query,\n            'start_time': datetime.utcnow().isoformat(),\n            'phases': {\n                'planning': {'status': 'failed'},\n                'execution': {'status': 'failed'},\n                'synthesis': {'status': 'failed'}\n            },\n            'errors': [error_message],\n            'final_response': {\n                'answer': 'I encountered an error while processing your query. Please try again.',\n                'confidence': 0.0,\n                'sources': [],\n                'error': error_message\n            },\n            'fallback': True\n        }\n    \n    def get_agent_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive agent statistics.\"\"\"\n        try:\n            routing_stats = self.routing_agent.get_routing_statistics()\n            planning_stats = self.planning_agent.get_planning_statistics()\n            react_stats = self.react_agent.get_react_statistics()\n            \n            return {\n                'routing_agent': routing_stats,\n                'planning_agent': planning_stats,\n                'react_agent': react_stats,\n                'execution_config': {\n                    'max_parallel_tasks': self.max_parallel_tasks,\n                    'execution_timeout': self.execution_timeout,\n                    'retry_attempts': self.retry_attempts\n                },\n                'supported_features': [\n                    'multi_step_planning',\n                    'parallel_execution',\n                    'real_time_data_integration',\n                    'dynamic_plan_adjustment',\n                    'comprehensive_synthesis'\n                ]\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error getting agent statistics: {e}\")\n            return {}

